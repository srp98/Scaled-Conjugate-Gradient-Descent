# Scaled-Conjugate-Gradient-Descent

This is a python-3 implementation of scaled conjugate gradient for neural networks, forward and backprop implemented from scratch using 
Numpy library. The weights are initialized using the Nyugen-Widrow paper. The papers talking about are in the Reference section.

As per the paper mentioned this type of descent algorithm works really well for upto 3 layers and can be faster, anything more than that
would make the training much worse and slower leading to extreme overfitting of the data.

# Requirements

Dependies for running or using the training algorithms:

- Numpy
